{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp src.rgxlog_interpreter.src.rgxlog.engine.session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from fastcore.basics import patch_to\n",
    "from typing import Tuple, List, Union, Optional, Callable, Type, Iterable, no_type_check, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lark in /miniconda/lib/python3.8/site-packages (1.1.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "!pip install lark   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /miniconda/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /miniconda/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /miniconda/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /miniconda/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /miniconda/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /miniconda/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tabulate in /miniconda/lib/python3.8/site-packages (0.9.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "!pip install pandas\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from lark.lark import Lark\n",
    "from pandas import DataFrame\n",
    "from tabulate import tabulate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installation NLP failed\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine import *\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.datatypes.ast_node_types import AddFact, RelationDeclaration\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.datatypes.primitive_types import Span, DataTypes, DataTypeMapping\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.engine import FALSE_VALUE, TRUE_VALUE\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.execution import (Query, FREE_VAR_PREFIX, naive_execution)\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.passes.adding_inference_rules_to_term_graph import AddRulesToTermGraph\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.passes.optimizations_passes import RemoveUselessRelationsFromRule\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.passes.lark_passes import (RemoveTokens, FixStrings, CheckReservedRelationNames,\n",
    "                                              ConvertSpanNodesToSpanInstances, ConvertStatementsToStructuredNodes,\n",
    "                                              CheckDefinedReferencedVariables,\n",
    "                                              CheckReferencedRelationsExistenceAndArity,\n",
    "                                              CheckReferencedIERelationsExistenceAndArity, CheckRuleSafety,\n",
    "                                              TypeCheckAssignments, TypeCheckRelations,\n",
    "                                              SaveDeclaredRelationsSchemas, ResolveVariablesReferences,\n",
    "                                              ExecuteAssignments, AddStatementsToNetxParseGraph, GenericPass)\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.state.graphs import TermGraph, NetxStateGraph, GraphBase, TermGraphBase\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.state.symbol_table import SymbolTable, SymbolTableBase\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.utils.general_utils import rule_to_relation_name, string_to_span, SPAN_PATTERN, QUERY_RESULT_PREFIX\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.engine.utils.passes_utils import LarkNode\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.stdlib.json_path import JsonPath, JsonPathFull\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.stdlib.nlp import (Tokenize, SSplit, POS, Lemma, NER, EntityMentions, CleanXML, Parse, DepParse, Coref,\n",
    "                              OpenIE, KBP, Quote, Sentiment, TrueCase)\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.stdlib.python_regex import PYRGX, PYRGX_STRING\n",
    "from spanner_workbench.src.rgxlog_interpreter.src.rgxlog.stdlib.rust_spanner_regex import RGX, RGX_STRING, RGX_FROM_FILE, RGX_STRING_FROM_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| exec_doc\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| exec_doc\n",
    "imported_classes = []\n",
    "for name, obj in list(globals().items()):\n",
    "    if inspect.isclass(obj) and inspect.getmodule(obj) is not None:\n",
    "        imported_classes.append(obj)\n",
    "for cls in imported_classes:\n",
    "    if 'spanner_workbench' in cls.__module__:\n",
    "        cls.__module__ = cls.__module__.split('.')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "CSV_DELIMITER = \";\"\n",
    "\n",
    "# ordered by rgx, json, nlp, etc.\n",
    "PREDEFINED_IE_FUNCS = [PYRGX, PYRGX_STRING, RGX, RGX_STRING, RGX_FROM_FILE, RGX_STRING_FROM_FILE,\n",
    "                       JsonPath, JsonPathFull,\n",
    "                       Tokenize, SSplit, POS, Lemma, NER, EntityMentions, CleanXML, Parse, DepParse, Coref, OpenIE, KBP, Quote, Sentiment,\n",
    "                       TrueCase]\n",
    "\n",
    "STRING_PATTERN = re.compile(r\"^[^\\r\\n]+$\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GRAMMAR_FILE_NAME = 'grammar.lark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _infer_relation_type(row: Iterable # an iterable of values, extracted from a csv file or a dataframe\n",
    "                        ) -> Sequence[DataTypes]: # Inferred tpye list of the given relation\n",
    "    \"\"\"\n",
    "    Guess the relation type based on the data.\n",
    "    We support both the actual types (e.g. 'Span'), and their string representation ( e.g. `\"[0,8)\"`).\n",
    "\n",
    "    **@raise** ValueError: if there is a cell inside `row` of an illegal type.\n",
    "    \"\"\"\n",
    "    relation_types = []\n",
    "    for cell in row:\n",
    "        try:\n",
    "            int(cell)  # check if the cell can be converted to integer\n",
    "            relation_types.append(DataTypes.integer)\n",
    "        except (ValueError, TypeError):\n",
    "            if isinstance(cell, Span) or re.match(SPAN_PATTERN, cell):\n",
    "                relation_types.append(DataTypes.span)\n",
    "            elif re.match(STRING_PATTERN, cell):\n",
    "                relation_types.append(DataTypes.string)\n",
    "            else:\n",
    "                raise ValueError(f\"value doesn't match any datatype: {cell}\")\n",
    "\n",
    "    return relation_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _verify_relation_types(row: Iterable, expected_types: Iterable[DataTypes]) -> None:\n",
    "    if _infer_relation_type(row) != expected_types:\n",
    "        raise Exception(f\"row:\\n{str(row)}\\ndoes not match the relation's types:\\n{str(expected_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _text_to_typed_data(term_list: Sequence[DataTypeMapping.term], relation_types: Sequence[DataTypes]) -> List[DataTypeMapping.term]:\n",
    "    transformed_term_list: List[DataTypeMapping.term] = []\n",
    "    for str_or_object, rel_type in zip(term_list, relation_types):\n",
    "        if rel_type == DataTypes.span:\n",
    "            if isinstance(str_or_object, Span):\n",
    "                transformed_term_list.append(str_or_object)\n",
    "            else:\n",
    "                assert isinstance(str_or_object, str), \"a span can only be a Span object or a string\"\n",
    "                transformed_span = string_to_span(str_or_object)\n",
    "                if transformed_span is None:\n",
    "                    raise TypeError(f\"expected a Span, found this instead: {str_or_object}\")\n",
    "                transformed_term_list.append(transformed_span)\n",
    "\n",
    "        elif rel_type == DataTypes.integer:\n",
    "            if isinstance(str_or_object, Span):\n",
    "                raise TypeError(f\"expected an int, found Span instead: {str_or_object}\")\n",
    "            transformed_term_list.append(int(str_or_object))\n",
    "        else:\n",
    "            assert rel_type == DataTypes.string, f\"illegal type given: {rel_type}\"\n",
    "            transformed_term_list.append(str_or_object)\n",
    "\n",
    "    return transformed_term_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_query_results(query: Query, query_results: List) -> Union[DataFrame, List]:\n",
    "    \"\"\"\n",
    "    Formats a single result from the engine into a usable format.\n",
    "\n",
    "    @param query: the query that was executed, and outputted `query_results`.\n",
    "    @param query_results: the results after executing the aforementioned query.\n",
    "    @return: a false value, a true value, or a dataframe representing the query + its results.\n",
    "    \"\"\"\n",
    "    assert isinstance(query_results, list), \"illegal results format\"\n",
    "\n",
    "    # check for the special conditions for which we can't print a table: no results were returned or a single\n",
    "    # empty tuple was returned\n",
    "\n",
    "    if query_results == FALSE_VALUE:  # empty list := false\n",
    "        return FALSE_VALUE\n",
    "    elif query_results == TRUE_VALUE:  # single tuple := true\n",
    "        return TRUE_VALUE\n",
    "    else:\n",
    "        # convert the resulting tuples to a more organized format\n",
    "        results_matrix = []\n",
    "        for result in query_results:\n",
    "            # span tuples are converted to Span objects\n",
    "            converted_span_result = [Span(term[0], term[1]) if (isinstance(term, tuple) and len(term) == 2)\n",
    "                                     else term\n",
    "                                     for term in result]\n",
    "\n",
    "            results_matrix.append(converted_span_result)\n",
    "\n",
    "        # get the free variables of the query, they will be used as headers\n",
    "        query_free_vars = [term for term, term_type in zip(query.term_list, query.type_list)\n",
    "                           if term_type is DataTypes.free_var_name]\n",
    "\n",
    "        return DataFrame(data=results_matrix, columns=query_free_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tabulate_result(result: Union[DataFrame, List]) -> str:\n",
    "    \"\"\"\n",
    "    Organizes a query result in a table\n",
    "    for example:\n",
    "        {QUERY_RESULT_PREFIX}'lecturer_of(X, \"abigail\")':\n",
    "          X\n",
    "       -------\n",
    "        linus\n",
    "        walter\n",
    "\n",
    "    there are two cases where a table will not be printed:\n",
    "    1. the query returned no results. in this case '[]' will be printed\n",
    "    2. the query returned a single empty tuple, in this case '[()]' will be printed\n",
    "\n",
    "    @param result: the query result (free variable names are the dataframe's column names).\n",
    "    @return: a tabulated string.\n",
    "    \"\"\"\n",
    "    if isinstance(result, DataFrame):\n",
    "        # query results can be printed as a table\n",
    "        result_string = tabulate(result, headers=\"keys\", tablefmt=\"presto\", stralign=\"center\", showindex=False)\n",
    "    else:\n",
    "        assert isinstance(result, list), \"illegal result format\"\n",
    "        if len(result) == 0:\n",
    "            result_string = \"[]\"\n",
    "        else:\n",
    "            assert len(result) == 1, \"illegal result format\"\n",
    "            result_string = \"[()]\"\n",
    "\n",
    "    return result_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def queries_to_string(query_results: List[Tuple[Query, List]]) -> str:\n",
    "    \"\"\"\n",
    "    Takes in a list of results from the engine and converts them into a single string, which contains\n",
    "    either a table, a false value (=`[]`), or a true value (=`[tuple()]`), for each result.\n",
    "\n",
    "    for example:\n",
    "\n",
    "    {QUERY_RESULT_PREFIX}'lecturer_of(X, \"abigail\")':\n",
    "      X\n",
    "    --------\n",
    "    linus\n",
    "    walter\n",
    "\n",
    "\n",
    "    @param query_results: List[the Query object used in execution, the execution's results (from engine)].\n",
    "    \"\"\"\n",
    "\n",
    "    all_result_strings = []\n",
    "    query_results = list(filter(None, query_results))  # remove Nones\n",
    "    for query, results in query_results:\n",
    "        query_result_string = tabulate_result(format_query_results(query, results))\n",
    "        query_title = f\"{QUERY_RESULT_PREFIX}'{query}':\"\n",
    "\n",
    "        # combine the title and table to a single string and save it to the prints buffer\n",
    "        titled_result_string = f'{query_title}\\n{query_result_string}\\n'\n",
    "        all_result_strings.append(titled_result_string)\n",
    "    return \"\\n\".join(all_result_strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Session:\n",
    "    def __init__(self, \n",
    "                 symbol_table: Optional[SymbolTableBase] = None, # symbol table to help with all semantic checks\n",
    "                 parse_graph: Optional[GraphBase] = None, # an AST that contains nodes which represent commands\n",
    "                 term_graph: Optional[TermGraphBase] = None): # a graph that holds all the connection between the relations\n",
    "        \"\"\"\n",
    "        A class that serves as the central connection point between various modules in the system.\n",
    "\n",
    "        This class takes input data and coordinates communication between different modules by sending the relevant parts\n",
    "        of the input to each module. It also orchestrates the execution of micro passes and handles engine-related tasks. <br>\n",
    "        Finally, it formats the results before presenting them to the user.\n",
    "\n",
    "        \"\"\"\n",
    "        if symbol_table is None:\n",
    "            self._symbol_table: SymbolTableBase = SymbolTable()\n",
    "            self._symbol_table.register_predefined_ie_functions(PREDEFINED_IE_FUNCS)\n",
    "\n",
    "        else:\n",
    "            self._symbol_table = symbol_table\n",
    "\n",
    "        self._parse_graph = NetxStateGraph() if parse_graph is None else parse_graph\n",
    "        self._term_graph: TermGraphBase = TermGraph() if term_graph is None else term_graph\n",
    "        self._engine = SqliteEngine()\n",
    "        self._execution = naive_execution\n",
    "\n",
    "        self._pass_stack: List[Type[GenericPass]] = [\n",
    "            RemoveTokens,\n",
    "            FixStrings,\n",
    "            CheckReservedRelationNames,\n",
    "            ConvertSpanNodesToSpanInstances,\n",
    "            ConvertStatementsToStructuredNodes,\n",
    "            CheckDefinedReferencedVariables,\n",
    "            CheckReferencedRelationsExistenceAndArity,\n",
    "            CheckReferencedIERelationsExistenceAndArity,\n",
    "            CheckRuleSafety,\n",
    "            TypeCheckAssignments,\n",
    "            TypeCheckRelations,\n",
    "            SaveDeclaredRelationsSchemas,\n",
    "            ResolveVariablesReferences,\n",
    "            ExecuteAssignments,\n",
    "            AddStatementsToNetxParseGraph,\n",
    "            AddRulesToTermGraph\n",
    "        ]\n",
    "\n",
    "        self._grammar = Session._get_grammar_from_file()\n",
    "\n",
    "        self._parser = Lark(self._grammar, parser='lalr')\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_grammar_from_file() -> str:\n",
    "        \"\"\"\n",
    "        @return: Grammar from grammar file in string format.\n",
    "        \"\"\"\n",
    "\n",
    "        grammar_file_path = Path(os.path.join(os.getcwd(),'..','spanner_workbench','src','rgxlog_interpreter','src','rgxlog','grammar'))\n",
    "        with open(grammar_file_path / GRAMMAR_FILE_NAME, 'r') as grammar_file:\n",
    "            return grammar_file.read()\n",
    "\n",
    "    # def _run_passes(self, lark_tree: LarkNode, pass_list: list) -> None:\n",
    "    #     \"\"\"\n",
    "    #     Runs the passes in pass_list on tree, one after another.\n",
    "    #     \"\"\"\n",
    "    #     #logger.debug(f\"initial lark tree:\\n{lark_tree.pretty()}\")\n",
    "    #     #logger.debug(f\"initial term graph:\\n{self._term_graph}\")\n",
    "\n",
    "    #     for curr_pass in pass_list:\n",
    "    #         curr_pass_object = curr_pass(parse_graph=self._parse_graph,\n",
    "    #                                      symbol_table=self._symbol_table,\n",
    "    #                                      term_graph=self._term_graph)\n",
    "    #         new_tree = curr_pass_object.run_pass(tree=lark_tree)\n",
    "    #         if new_tree is not None:\n",
    "    #             lark_tree = new_tree\n",
    "    #             #logger.debug(f\"lark tree after {curr_pass.__name__}:\\n{lark_tree.pretty()}\")\n",
    "    \n",
    "    \n",
    "    # def __repr__(self) -> str:\n",
    "    #     return \"\\n\".join([repr(self._symbol_table), repr(self._parse_graph)])\n",
    "    \n",
    "    # def __str__(self) -> str:\n",
    "    #     return f'Symbol Table:\\n{str(self._symbol_table)}\\n\\nTerm Graph:\\n{str(self._parse_graph)}'\n",
    "    \n",
    "    # def run_commands(self, query: str, print_results: bool = True, format_results: bool = False) -> (\n",
    "    #     Union[List[Union[List, List[Tuple], DataFrame]], List[Tuple[Query, List]]]):\n",
    "    #     \"\"\"\n",
    "    #     Generates an AST and passes it through the pass stack.\n",
    "\n",
    "    #     @param format_results: if this is true, return the formatted result instead of the `[Query, List]` pair.\n",
    "    #     @param query: the user's input.\n",
    "    #     @param print_results: whether to print the results to stdout or not.\n",
    "    #     @return: the results of every query, in a list.\n",
    "    #     \"\"\"\n",
    "    #     query_results = []\n",
    "    #     parse_tree = self._parser.parse(query)\n",
    "    #     for statement in parse_tree.children:\n",
    "    #         self._run_passes(statement, self._pass_stack)\n",
    "    #         query_result = self._execution(parse_graph=self._parse_graph,\n",
    "    #                                        symbol_table=self._symbol_table,\n",
    "    #                                        rgxlog_engine=self._engine,\n",
    "    #                                        term_graph=self._term_graph)\n",
    "    #         if query_result is not None:\n",
    "    #             query_results.append(query_result)\n",
    "    #             if print_results:\n",
    "    #                 print(queries_to_string([query_result]))\n",
    "\n",
    "    #     if format_results:\n",
    "    #         return [format_query_results(*query_result) for query_result in query_results]\n",
    "    #     else:\n",
    "    #         return query_results\n",
    "        \n",
    "\n",
    "    # def register(self, ie_function: Callable, ie_function_name: str, in_rel: List[DataTypes],\n",
    "    #             out_rel: Union[List[DataTypes], Callable[[int], Sequence[DataTypes]]]) -> None:\n",
    "    #     \"\"\"\n",
    "    #     Registers an ie function.\n",
    "\n",
    "    #     @see params in IEFunction's __init__.\n",
    "    #     \"\"\"\n",
    "    #     self._symbol_table.register_ie_function(ie_function, ie_function_name, in_rel, out_rel)\n",
    "\n",
    "    # def get_pass_stack(self) -> List[Type[GenericPass]]:\n",
    "    #     \"\"\"\n",
    "    #     @return: the current pass stack.\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     return self._pass_stack.copy()\n",
    "    \n",
    "\n",
    "    # def set_pass_stack(self, user_stack: List[Type[GenericPass]]) -> List[Type[GenericPass]]:\n",
    "    #     \"\"\"\n",
    "    #     Sets a new pass stack instead of the current one.\n",
    "\n",
    "    #     @param user_stack: a user supplied pass stack.\n",
    "    #     @return: success message with the new pass stack.\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     if type(user_stack) is not list:\n",
    "    #         raise TypeError('user stack should be a list of passes')\n",
    "    #     for pass_ in user_stack:\n",
    "    #         if not issubclass(pass_, GenericPass):\n",
    "    #             raise TypeError('user stack should be a subclass of `GenericPass`')\n",
    "\n",
    "    #     self._pass_stack = user_stack.copy()\n",
    "    #     return self.get_pass_stack()\n",
    "\n",
    "    # def _remove_rule_relation_from_symbols_and_engine(self, relation_name: str) -> None:\n",
    "    #     \"\"\"\n",
    "    #     Removes the relation from the symbol table and the execution tables.\n",
    "\n",
    "    #     @param relation_name: the name of the relation ot remove.\n",
    "    #     \"\"\"\n",
    "    #     self._symbol_table.remove_rule_relation(relation_name)\n",
    "    #     self._engine.remove_table(relation_name)\n",
    "        \n",
    "    # def remove_rule(self, rule: str) -> None:\n",
    "    #     \"\"\"\n",
    "    #     Remove a rule from the rgxlog's engine.\n",
    "\n",
    "    #     @param rule: the rule to be removed.\n",
    "    #     \"\"\"\n",
    "    #     is_last = self._term_graph.remove_rule(rule)\n",
    "    #     if is_last:\n",
    "    #         relation_name = rule_to_relation_name(rule)\n",
    "    #         self._remove_rule_relation_from_symbols_and_engine(relation_name)\n",
    "            \n",
    "    # def remove_all_rules(self, rule_head: Optional[str] = None) -> None:\n",
    "    #     \"\"\"\n",
    "    #     Removes all rules from the engine.\n",
    "\n",
    "    #     @param rule_head: if rule head is not none we remove all rules with rule_head.\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     if rule_head is None:\n",
    "    #         self._term_graph = TermGraph()\n",
    "    #         relations_names = self._symbol_table.remove_all_rule_relations()\n",
    "    #         self._engine.remove_tables(relations_names)\n",
    "    #     else:\n",
    "    #         self._term_graph.remove_rules_with_head(rule_head)\n",
    "    #         self._remove_rule_relation_from_symbols_and_engine(rule_head)\n",
    "            \n",
    "    # def _add_imported_relation_to_engine(self, relation_table: Iterable, relation_name: str, relation_types: Sequence[DataTypes]) -> None:\n",
    "    #     symbol_table = self._symbol_table\n",
    "    #     engine = self._engine\n",
    "    #     # first make sure the types are legal, then we add them to the engine (to make sure\n",
    "    #     #  we don't add them in case of an error)\n",
    "    #     facts = []\n",
    "\n",
    "    #     for row in relation_table:\n",
    "    #         _verify_relation_types(row, relation_types)\n",
    "    #         typed_line = _text_to_typed_data(row, relation_types)\n",
    "    #         facts.append(AddFact(relation_name, typed_line, relation_types))\n",
    "\n",
    "    #     # declare relation if it does not exist\n",
    "    #     if not symbol_table.contains_relation(relation_name):\n",
    "    #         engine.declare_relation_table(RelationDeclaration(relation_name, relation_types))\n",
    "    #         symbol_table.add_relation_schema(relation_name, relation_types, False)\n",
    "\n",
    "    #     for fact in facts:\n",
    "    #         engine.add_fact(fact)\n",
    "            \n",
    "    # def import_relation_from_csv(self, csv_file_name: Path, relation_name: str = None, delimiter: str = CSV_DELIMITER) -> None:\n",
    "    #     if not Path(csv_file_name).is_file():\n",
    "    #         raise IOError(\"csv file does not exist\")\n",
    "\n",
    "    #     if os.stat(csv_file_name).st_size == 0:\n",
    "    #         raise IOError(\"csv file is empty\")\n",
    "\n",
    "    #     # the relation_name is either an argument or the file's name\n",
    "    #     if relation_name is None:\n",
    "    #         relation_name = Path(csv_file_name).stem\n",
    "\n",
    "    #     with open(csv_file_name) as fh:\n",
    "    #         reader = csv.reader(fh, delimiter=delimiter)\n",
    "\n",
    "    #         # read first line and go back to start of file - make sure there is no empty line!\n",
    "    #         relation_types = _infer_relation_type(next(reader))\n",
    "    #         fh.seek(0)\n",
    "\n",
    "    #         self._add_imported_relation_to_engine(reader, relation_name, relation_types)\n",
    "            \n",
    "    # def import_relation_from_df(self, relation_df: DataFrame, relation_name: str) -> None:\n",
    "    #     data = relation_df.values.tolist()\n",
    "\n",
    "    #     if not isinstance(data, list):\n",
    "    #         raise Exception(\"dataframe could not be converted to list\")\n",
    "\n",
    "    #     if len(data) < 1:\n",
    "    #         raise Exception(\"dataframe is empty\")\n",
    "\n",
    "    #     relation_types = _infer_relation_type(data[0])\n",
    "\n",
    "    #     self._add_imported_relation_to_engine(data, relation_name, relation_types)\n",
    "        \n",
    "    # def send_commands_result_into_csv(self, commands: str, csv_file_name: Path, delimiter: str = CSV_DELIMITER) -> None:\n",
    "    #     \"\"\"\n",
    "    #     run commands as usual and output their formatted results into a csv file (the commands should contain a query)\n",
    "    #     @param commands: the commands to run\n",
    "    #     @param csv_file_name: the file into which the output will be written\n",
    "    #     @param delimiter: a csv separator between values\n",
    "    #     @return: None\n",
    "    #     \"\"\"\n",
    "    #     commands_results = self.run_commands(commands, print_results=False)\n",
    "    #     if len(commands_results) != 1:\n",
    "    #         raise Exception(\"the commands must have exactly one output\")\n",
    "\n",
    "    #     formatted_result = format_query_results(*commands_results[0])\n",
    "\n",
    "    #     if isinstance(formatted_result, DataFrame):\n",
    "    #         formatted_result.to_csv(csv_file_name, index=False, sep=delimiter)\n",
    "    #     else:\n",
    "    #         # true or false\n",
    "    #         with open(csv_file_name, \"w\", newline=\"\") as f:\n",
    "    #             writer = csv.writer(f, delimiter=delimiter)\n",
    "    #             writer.writerows(formatted_result)\n",
    "                \n",
    "    # def send_commands_result_into_df(self, commands: str) -> Union[DataFrame, List]:\n",
    "    #     \"\"\"\n",
    "    #     run commands as usual and output their formatted results into a dataframe (the commands should contain a query)\n",
    "    #     @param commands: the commands to run\n",
    "    #     @return: formatted results (possibly a dataframe)\n",
    "    #     \"\"\"\n",
    "    #     commands_results = self.run_commands(commands, print_results=False)\n",
    "    #     if len(commands_results) != 1:\n",
    "    #         raise Exception(\"the commands must have exactly one output\")\n",
    "\n",
    "    #     return format_query_results(*commands_results[0])\n",
    "    \n",
    "    # def _relation_name_to_query(self, relation_name: str) -> str:\n",
    "    #     symbol_table = self._symbol_table\n",
    "    #     relation_schema = symbol_table.get_relation_schema(relation_name)\n",
    "    #     relation_arity = len(relation_schema)\n",
    "    #     query = (f\"?{relation_name}(\" + \", \".join(f\"{FREE_VAR_PREFIX}{i}\" for i in range(relation_arity)) + \")\")\n",
    "    #     return query\n",
    "    \n",
    "    # def export_relation_into_df(self, relation_name: str) -> Union[DataFrame, List]:\n",
    "    #     query = self._relation_name_to_query(relation_name)\n",
    "    #     return self.send_commands_result_into_df(query)\n",
    "\n",
    "    # def export_relation_into_csv(self, csv_file_name: Path, relation_name: str, delimiter: str = CSV_DELIMITER) -> None:\n",
    "    #     query = self._relation_name_to_query(relation_name)\n",
    "    #     self.send_commands_result_into_csv(query, csv_file_name, delimiter)\n",
    "        \n",
    "    # def print_registered_ie_functions(self) -> None:\n",
    "    #     \"\"\"\n",
    "    #     Prints information about the registered ie functions.\n",
    "    #     \"\"\"\n",
    "    #     self._symbol_table.print_registered_ie_functions()\n",
    "        \n",
    "    # def remove_ie_function(self, name: str) -> None:\n",
    "    #     \"\"\"\n",
    "    #     Removes a function from the symbol table.\n",
    "\n",
    "    #     @param name: the name of the ie function to remove.\n",
    "    #     \"\"\"\n",
    "    #     self._symbol_table.remove_ie_function(name)\n",
    "    # def remove_all_ie_functions(self) -> None:\n",
    "    #     \"\"\"\n",
    "    #     Removes all the ie functions from the symbol table.\n",
    "    #     \"\"\"\n",
    "    #     self._symbol_table.remove_all_ie_functions()\n",
    "        \n",
    "    # def print_all_rules(self, head: Optional[str] = None) -> None:\n",
    "    #     \"\"\"\n",
    "    #     Prints all the rules that are registered.\n",
    "\n",
    "    #     @param head: if specified it will print only rules with the given head relation name.\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     self._term_graph.print_all_rules(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def _run_passes(self, lark_tree: LarkNode, pass_list: list) -> None:\n",
    "    \"\"\"\n",
    "    Runs the passes in pass_list on tree, one after another.\n",
    "    \"\"\"\n",
    "    #logger.debug(f\"initial lark tree:\\n{lark_tree.pretty()}\")\n",
    "    #logger.debug(f\"initial term graph:\\n{self._term_graph}\")\n",
    "\n",
    "    for curr_pass in pass_list:\n",
    "        curr_pass_object = curr_pass(parse_graph=self._parse_graph,\n",
    "                                        symbol_table=self._symbol_table,\n",
    "                                        term_graph=self._term_graph)\n",
    "        new_tree = curr_pass_object.run_pass(tree=lark_tree)\n",
    "        if new_tree is not None:\n",
    "            lark_tree = new_tree\n",
    "            #logger.debug(f\"lark tree after {curr_pass.__name__}:\\n{lark_tree.pretty()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def __repr__(self) -> str:\n",
    "    return \"\\n\".join([repr(self._symbol_table), repr(self._parse_graph)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def __str__(self) -> str:\n",
    "    return f'Symbol Table:\\n{str(self._symbol_table)}\\n\\nTerm Graph:\\n{str(self._parse_graph)}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def run_commands(self, query: str, print_results: bool = True, format_results: bool = False) -> (\n",
    "    Union[List[Union[List, List[Tuple], DataFrame]], List[Tuple[Query, List]]]):\n",
    "    \"\"\"\n",
    "    Generates an AST and passes it through the pass stack.\n",
    "\n",
    "    @param format_results: if this is true, return the formatted result instead of the `[Query, List]` pair.\n",
    "    @param query: the user's input.\n",
    "    @param print_results: whether to print the results to stdout or not.\n",
    "    @return: the results of every query, in a list.\n",
    "    \"\"\"\n",
    "    query_results = []\n",
    "    parse_tree = self._parser.parse(query)\n",
    "    for statement in parse_tree.children:\n",
    "        self._run_passes(statement, self._pass_stack)\n",
    "        query_result = self._execution(parse_graph=self._parse_graph,\n",
    "                                        symbol_table=self._symbol_table,\n",
    "                                        rgxlog_engine=self._engine,\n",
    "                                        term_graph=self._term_graph)\n",
    "        if query_result is not None:\n",
    "            query_results.append(query_result)\n",
    "            if print_results:\n",
    "                print(queries_to_string([query_result]))\n",
    "\n",
    "    if format_results:\n",
    "        return [format_query_results(*query_result) for query_result in query_results]\n",
    "    else:\n",
    "        return query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def register(self, ie_function: Callable, ie_function_name: str, in_rel: List[DataTypes],\n",
    "            out_rel: Union[List[DataTypes], Callable[[int], Sequence[DataTypes]]]) -> None:\n",
    "    \"\"\"\n",
    "    Registers an ie function.\n",
    "\n",
    "    @see params in IEFunction's __init__.\n",
    "    \"\"\"\n",
    "    self._symbol_table.register_ie_function(ie_function, ie_function_name, in_rel, out_rel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def get_pass_stack(self) -> List[Type[GenericPass]]:\n",
    "    \"\"\"\n",
    "    @return: the current pass stack.\n",
    "    \"\"\"\n",
    "\n",
    "    return self._pass_stack.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def set_pass_stack(self, user_stack: List[Type[GenericPass]]) -> List[Type[GenericPass]]:\n",
    "    \"\"\"\n",
    "    Sets a new pass stack instead of the current one.\n",
    "\n",
    "    @param user_stack: a user supplied pass stack.\n",
    "    @return: success message with the new pass stack.\n",
    "    \"\"\"\n",
    "\n",
    "    if type(user_stack) is not list:\n",
    "        raise TypeError('user stack should be a list of passes')\n",
    "    for pass_ in user_stack:\n",
    "        if not issubclass(pass_, GenericPass):\n",
    "            raise TypeError('user stack should be a subclass of `GenericPass`')\n",
    "\n",
    "    self._pass_stack = user_stack.copy()\n",
    "    return self.get_pass_stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def _remove_rule_relation_from_symbols_and_engine(self, relation_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Removes the relation from the symbol table and the execution tables.\n",
    "\n",
    "    @param relation_name: the name of the relation ot remove.\n",
    "    \"\"\"\n",
    "    self._symbol_table.remove_rule_relation(relation_name)\n",
    "    self._engine.remove_table(relation_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def remove_rule(self, rule: str) -> None:\n",
    "    \"\"\"\n",
    "    Remove a rule from the rgxlog's engine.\n",
    "\n",
    "    @param rule: the rule to be removed.\n",
    "    \"\"\"\n",
    "    is_last = self._term_graph.remove_rule(rule)\n",
    "    if is_last:\n",
    "        relation_name = rule_to_relation_name(rule)\n",
    "        self._remove_rule_relation_from_symbols_and_engine(relation_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def remove_all_rules(self, rule_head: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Removes all rules from the engine.\n",
    "\n",
    "    @param rule_head: if rule head is not none we remove all rules with rule_head.\n",
    "    \"\"\"\n",
    "\n",
    "    if rule_head is None:\n",
    "        self._term_graph = TermGraph()\n",
    "        relations_names = self._symbol_table.remove_all_rule_relations()\n",
    "        self._engine.remove_tables(relations_names)\n",
    "    else:\n",
    "        self._term_graph.remove_rules_with_head(rule_head)\n",
    "        self._remove_rule_relation_from_symbols_and_engine(rule_head)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def _add_imported_relation_to_engine(self, relation_table: Iterable, relation_name: str, relation_types: Sequence[DataTypes]) -> None:\n",
    "    symbol_table = self._symbol_table\n",
    "    engine = self._engine\n",
    "    # first make sure the types are legal, then we add them to the engine (to make sure\n",
    "    #  we don't add them in case of an error)\n",
    "    facts = []\n",
    "\n",
    "    for row in relation_table:\n",
    "        _verify_relation_types(row, relation_types)\n",
    "        typed_line = _text_to_typed_data(row, relation_types)\n",
    "        facts.append(AddFact(relation_name, typed_line, relation_types))\n",
    "\n",
    "    # declare relation if it does not exist\n",
    "    if not symbol_table.contains_relation(relation_name):\n",
    "        engine.declare_relation_table(RelationDeclaration(relation_name, relation_types))\n",
    "        symbol_table.add_relation_schema(relation_name, relation_types, False)\n",
    "\n",
    "    for fact in facts:\n",
    "        engine.add_fact(fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def import_relation_from_csv(self, csv_file_name: Path, relation_name: str = None, delimiter: str = CSV_DELIMITER) -> None:\n",
    "    if not Path(csv_file_name).is_file():\n",
    "        raise IOError(\"csv file does not exist\")\n",
    "\n",
    "    if os.stat(csv_file_name).st_size == 0:\n",
    "        raise IOError(\"csv file is empty\")\n",
    "\n",
    "    # the relation_name is either an argument or the file's name\n",
    "    if relation_name is None:\n",
    "        relation_name = Path(csv_file_name).stem\n",
    "\n",
    "    with open(csv_file_name) as fh:\n",
    "        reader = csv.reader(fh, delimiter=delimiter)\n",
    "\n",
    "        # read first line and go back to start of file - make sure there is no empty line!\n",
    "        relation_types = _infer_relation_type(next(reader))\n",
    "        fh.seek(0)\n",
    "\n",
    "        self._add_imported_relation_to_engine(reader, relation_name, relation_types)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def import_relation_from_df(self, relation_df: DataFrame, relation_name: str) -> None:\n",
    "    data = relation_df.values.tolist()\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        raise Exception(\"dataframe could not be converted to list\")\n",
    "\n",
    "    if len(data) < 1:\n",
    "        raise Exception(\"dataframe is empty\")\n",
    "\n",
    "    relation_types = _infer_relation_type(data[0])\n",
    "\n",
    "    self._add_imported_relation_to_engine(data, relation_name, relation_types)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def send_commands_result_into_csv(self, commands: str, csv_file_name: Path, delimiter: str = CSV_DELIMITER) -> None:\n",
    "    \"\"\"\n",
    "    run commands as usual and output their formatted results into a csv file (the commands should contain a query)\n",
    "    @param commands: the commands to run\n",
    "    @param csv_file_name: the file into which the output will be written\n",
    "    @param delimiter: a csv separator between values\n",
    "    @return: None\n",
    "    \"\"\"\n",
    "    commands_results = self.run_commands(commands, print_results=False)\n",
    "    if len(commands_results) != 1:\n",
    "        raise Exception(\"the commands must have exactly one output\")\n",
    "\n",
    "    formatted_result = format_query_results(*commands_results[0])\n",
    "\n",
    "    if isinstance(formatted_result, DataFrame):\n",
    "        formatted_result.to_csv(csv_file_name, index=False, sep=delimiter)\n",
    "    else:\n",
    "        # true or false\n",
    "        with open(csv_file_name, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f, delimiter=delimiter)\n",
    "            writer.writerows(formatted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def send_commands_result_into_df(self, commands: str) -> Union[DataFrame, List]:\n",
    "    \"\"\"\n",
    "    run commands as usual and output their formatted results into a dataframe (the commands should contain a query)\n",
    "    @param commands: the commands to run\n",
    "    @return: formatted results (possibly a dataframe)\n",
    "    \"\"\"\n",
    "    commands_results = self.run_commands(commands, print_results=False)\n",
    "    if len(commands_results) != 1:\n",
    "        raise Exception(\"the commands must have exactly one output\")\n",
    "\n",
    "    return format_query_results(*commands_results[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def _relation_name_to_query(self, relation_name: str) -> str:\n",
    "    symbol_table = self._symbol_table\n",
    "    relation_schema = symbol_table.get_relation_schema(relation_name)\n",
    "    relation_arity = len(relation_schema)\n",
    "    query = (f\"?{relation_name}(\" + \", \".join(f\"{FREE_VAR_PREFIX}{i}\" for i in range(relation_arity)) + \")\")\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def export_relation_into_df(self, relation_name: str) -> Union[DataFrame, List]:\n",
    "    query = self._relation_name_to_query(relation_name)\n",
    "    return self.send_commands_result_into_df(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def export_relation_into_csv(self, csv_file_name: Path, relation_name: str, delimiter: str = CSV_DELIMITER) -> None:\n",
    "    query = self._relation_name_to_query(relation_name)\n",
    "    self.send_commands_result_into_csv(query, csv_file_name, delimiter)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def print_registered_ie_functions(self) -> None:\n",
    "    \"\"\"\n",
    "    Prints information about the registered ie functions.\n",
    "    \"\"\"\n",
    "    self._symbol_table.print_registered_ie_functions()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def remove_ie_function(self, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Removes a function from the symbol table.\n",
    "\n",
    "    @param name: the name of the ie function to remove.\n",
    "    \"\"\"\n",
    "    self._symbol_table.remove_ie_function(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def remove_all_ie_functions(self) -> None:\n",
    "    \"\"\"\n",
    "    Removes all the ie functions from the symbol table.\n",
    "    \"\"\"\n",
    "    self._symbol_table.remove_all_ie_functions()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch_to(Session)\n",
    "def print_all_rules(self, head: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Prints all the rules that are registered.\n",
    "\n",
    "    @param head: if specified it will print only rules with the given head relation name.\n",
    "    \"\"\"\n",
    "\n",
    "    self._term_graph.print_all_rules(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing results for query 'a(Z)':\n",
      "  Z\n",
      "-----\n",
      "  A\n",
      "\n",
      "printing results for query 'grandparent(X, Y)':\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # this is for debugging. don't shadow variables like `query`, that's annoying\n",
    "    #logger = logging.getLogger()\n",
    "    #logger.setLevel(level=logging.DEBUG)\n",
    "    # logging.basicConfig(level=logging.DEBUG)\n",
    "    my_session = Session()\n",
    "    my_session.run_commands(\"\"\"\n",
    "                        new parent(str,str)\n",
    "                        new number(str)\n",
    "                        grandparent(X,Z) <- parent(X,Y), parent(Y,Z), parent(J,J), parent(K,M), number(X), number(Y), number(Q)\n",
    "                        ?grandparent(X,Y)\n",
    "            \"\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        \n",
    "                        # important_events(EVE, Y) <- event(EVE, Y), important_year(Y)\n",
    "                        # ?important_events(X,Y)\n",
    "                        # important_events_per_cet(EVE, CET) <- important_events(EVE, Y), which_century(Y) -> (CET)\n",
    "                        # ?important_events_per_cet(EVE, CET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
